# AllData 一站式细分领域数字化解决方案

## [原型](https://orgnext.modao.cc/app/HhitGZQTr954c7Ug8XBvAY) ｜ [官方文档](https://alldata.readthedocs.io/) ｜ [Document](https://github.com/alldatacenter/alldata/blob/master/README.md) ｜ [Community](#community)


## Stargazers over time

[![Stargazers over time](https://starchart.cc/alldatacenter/alldata.svg)](https://starchart.cc/alldatacenter/alldata)

<br/>
<br/>
<a href="https://github.com/alldatacenter/github-readme-stats">
  <img width="1215" align="center" src="https://github-readme-stats.vercel.app/api/pin/?username=alldatacenter&repo=alldata" />
</a>

## 社区版
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/204965509-fc13050b-ebe8-4bd5-8882-69e1af0a8367.png">
<br/>

## AllData数字化解决方案
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/235920344-fbf3c9d2-6239-4c73-aa9c-77a72773780e.png">
<br/>

## 首页

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227456470-6fd4e3c8-e5b7-48f9-bf3a-83975f78654c.png">
<br/>

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227456480-d23d6df9-65f0-45f8-8d02-b1a448fcf0cc.png">
<br/>

## 数据集成

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/206071504-c0e2b3ca-e3c2-4d70-8213-55c7316465ff.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/206071513-61997f31-12c4-464b-897d-bb60f0095ee9.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/206071519-35651c39-38c9-4746-ba18-c110a672e48f.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/206071527-d2af118c-ea4e-49f7-8435-c2b9ab8039cc.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/206071530-408d3001-0596-440c-93da-60d50aaacebe.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/206071537-4c0c2b38-dbfa-46a1-850e-4b73d5ca4236.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/206071539-f6b95712-4a32-481c-b719-f56c7542888c.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/206071544-43c5136d-091d-4602-b6de-ef528f39095c.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/206071554-1bc698f3-41d4-4fc5-ac74-a419ffede30e.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/206071565-1634a747-0f60-4dcd-a294-90d80d59cdc7.png">
<br/>

## 数据质量
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227457313-aac5a92e-3780-4b81-99f3-de6ea1b32f79.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227457303-9272dd14-6602-4f32-ab26-ea2c4b9e7c55.png">
<br/>

## 数据标准

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227457469-8183b633-ace2-48ed-91ac-9d1c114e7e67.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227457478-8ddf5083-5bad-4b27-90b9-14bebf7615f6.png">
<br/>

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227457492-9c90f9a6-2a56-47ce-9042-00e2c5c662ec.png">
<br/>

## 元数据管理

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227456662-be27764e-f2a1-46a1-97fb-4689a93a42a9.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227456676-9c66e22b-393c-4757-a22f-2aa1604bef51.png">
<br/>

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227456683-36436d97-76fc-4a0d-b4b7-e0090a6a2575.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227456694-06a042cf-cad6-4bf5-ae41-8f1fad46c642.png">
<br/>

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227456700-76a54b9f-e806-4ca7-b1d3-d0a9fa383de1.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227456710-c004ac4a-5f67-45f2-9cbf-67b855a55c34.png">
<br/>

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227456722-4f468872-2ada-4ad8-a56e-69c63f9b7e31.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227456740-12c5298d-c72f-46ec-88ec-c680efbb592f.png">
<br/>

## 数据资产
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227457692-c9c1a763-b0e5-4afe-abe7-623203ab6231.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227457706-872b49b5-9ec5-4ad3-beb1-535fbd88dde3.png">
<br/>


## 数据市场
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227457844-e1a3ebbe-5a7c-43c4-8317-2bf066c5deed.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227457858-37307d6b-41a1-4475-8d69-d7bcc38f6f10.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227457862-415bdb7f-784e-4db9-9e54-eb83ea7a581f.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227457872-6cbb24e0-82e1-47de-b3f8-200cdb47b90f.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227457882-77fc873b-c340-47f1-b41c-9b2d5e73ff4d.png">
<br/>

## 数据比对

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/234816803-ca89bdd2-bce6-499d-8dbf-0522ed07ba86.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/234816815-89864618-f786-47ff-ae31-4fd8c662130b.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/234816833-7bf1202b-9a9a-48cd-9523-d329ad92cf69.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/234816849-5563f3c1-32f3-4ab8-ae1a-7919c65fd656.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/234816860-72f06760-94cd-497f-a623-7bb378d8077f.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/234816915-d45771bb-de17-4e3c-88ae-183d1aa28a1b.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/234817116-dc4bca8e-e8ac-4efa-a2a0-d91ce9eb56e8.png">
<br/>

## BI报表
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227458114-48ab43b4-437d-4b7e-9eb3-d49b1da1b762.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227458126-d5367210-fd82-4190-bf71-b99c6fc3176e.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227458133-39ad2515-4447-4f6d-a9c7-e353d70c628d.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227458143-5c771b96-6888-48e1-9e1c-265efee62b30.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227458149-b658b9ea-c6df-4a7d-884c-d02f42aadb3e.png">
<br/>

## 流程编排
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227458401-9244442e-c66d-4ebe-8a81-e989bedb1415.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227458415-ec02039f-73d0-4513-b613-5aa931c9b294.png">
<br/>


## 系统监控
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227458501-7493df22-026b-402d-80dd-03c4b9bd8224.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227458517-45eb4fd4-ccc1-4a37-8a0d-4403301bbc55.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227458525-639b90bf-ffc3-439e-ba74-b9e9ce2f1323.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227458535-2fea5b2c-b478-4192-a303-f377de0cd9b1.png">
<br/>


## 运维管理
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227458729-f6517a7e-ad37-4ce1-94d8-e2a2b41de7b9.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227458745-5455a74e-706a-4498-af4a-39454f743cd6.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227458756-9db88ca2-848d-4461-a4da-5ca149288779.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227458765-f6be2c35-d870-4952-a79c-bb26ebaac6e7.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227458778-4f2e1b46-d532-4411-b104-8288fcb97ad7.png">
<br/>

## AllData AI Studio 社区版
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/211255550-2d58eb94-42ce-411c-9487-9f2e499e565a.png">
<br/>

## AllData Studio 社区版
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/210299541-b9f4d344-30ba-4fc9-a083-390129f7da1e.png">
<br/>

### 数据中台建设方法论

> 确定数据中台的目标和范围: 在开始构建数据中台之前, 需要明确数据中台的目标和范围, 例如数据中台的主要功能, 应用场景, 覆盖范围等
>
> 收集数据源: 根据定义的目标和范围, 收集相关的数据源这些数据源可以包括内部和外部的结构化和非结构化数据, 实时数据, 历史数据等
>
> 数据预处理: 对收集到的数据进行预处理, 包括数据清洗, 归一化, 去重, 脱敏等操作, 以确保数据的准确性, 一致性和安全性
>
> 数据建模和集成: 对预处理的数据进行建模和集成, 包括定义数据模型, 设计数据流程, 数据映射和转换等操作, 以确保数据的结构和语义一致性
>
> 数据存储和管理: 将集成的数据存储在数据仓库或数据湖中, 并实现数据的管理, 备份和恢复
>
> 数据治理和质量管理: 实现数据质量监控, 数据血缘追溯, 数据安全保障, 合规性检查等数据治理和质量管理功能, 以保证数据的高质量和可信性
>
> 数据分析和应用: 基于数据中台, 实现数据分析和应用, 如数据挖掘, 数据可视化, 机器学习等, 以提高数据的价值和应用效果
>
> 持续优化和创新: 数据中台的建设是一个持续迭代的过程, 需要不断进行优化和创新, 以适应不断变化的数据需求和业务场景
>
> 需要注意的是, 数据中台建设需要依赖于先进的技术和方法, 如云计算, 大数据, 人工智能, 数据湖等
>
> 同时, 数据中台建设还需要依赖于跨部门的协同和配合, 以确保数据的一致性和可用性

### 功能一览

- 平台基础设置
    - 系统管理
        - 岗位管理: 配置系统用户所属担任职务
        - 部门管理: 配置系统组织机构, 树结构展现支持数据权限
        - 菜单管理: 配置系统菜单, 操作权限, 按钮权限标识等
        - 角色管理: 角色菜单权限分配, 设置角色按机构进行数据范围权限划分
        - 用户管理: 用户是系统操作者, 该功能主要完成系统用户配置
        - 参数管理: 对系统动态配置常用参数
        - 字典管理: 对系统中经常使用的一些较为固定的数据进行维护
    - 系统监控
        - 登录日志: 系统登录日志记录查询
        - 操作日志: 系统正常操作日志记录和查询, 系统异常信息日志记录和查询
    - 任务调度
        - 任务管理: 在线（添加, 修改, 删除)任务调度
        - 日志管理: 任务调度执行结果日志
- 元数据管理
    - 数据源: 数据源连接信息管理, 可生成数据库文档
    - 元数据: 数据库表的元数据信息管理
    - 数据授权: 设置元数据信息权限划分
    - 变更记录: 元数据信息变更记录信息管理
    - 数据检索: 数据源, 数据表, 元数据等信息查询
    - 数据地图: 元数据的隶属数据表, 数据库的图形展示
    - SQL工作台: 在线执行查询sql
- 数据标准管理
    - 标准字典: 国标数据维护
    - 对照表: 本地数据中需要对照标准的数据维护
    - 字典对照: 本地数据与国标数据的对照关系
    - 对照统计: 本地数据与国标数据的对照结果统计分析
- 数据质量管理
    - 规则配置: 数据质量规则配置
    - 问题统计: 数据质量规则统计
    - 质量报告: 数据质量结果统计分析
    - 定时任务: 数据质量定时任务
    - 任务日志: 数据质量定时任务日志
- 主数据管理
    - 数据模型: 主数据数据模型维护
    - 数据管理: 主数据数据管理
- 数据集市管理
    - 数据服务: 动态开发api数据服务, 可生成数据服务文档
    - 数据脱敏: api数据服务返回结果动态脱敏
    - 接口日志: api数据服务调用日志
    - 服务集成: 三方数据服务集成管理
    - 服务日志: 三方数据服务集成调用日志
- 可视化管理
    - 数据集: 基于sql的查询结果维护
    - 图表配置: 动态echarts图表配置, 支持多维表格, 折线, 柱状, 饼图, 雷达, 散点等多种图表
    - 看板配置: 拖拽式添加图表组件, 调整位置, 大小
    - 酷屏配置: 拖拽式添加图表组件, 调整背景图, 颜色, 位置, 大小
- 流程管理
    - 流程定义: 流程定义管理
    - 流程实例
        - 运行中的流程: 运行中的流程实例管理
        - 我发起的流程: 我发起的流程实例管理
        - 我参与的流程: 我参与的流程实例管理
    - 流程任务
        - 待办任务: 待办任务管理
        - 已办任务: 已办任务管理
    - 业务配置: 配置业务系统与流程的相关属性

### 部署方式
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/226297187-d36d6ebf-9cdc-4e1a-81bb-860af018d14e.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/221345609-45a34a1a-8316-4810-8624-bc43a0e3c91d.png">
<br/>

| 16gmaster                | port | ip             |
|--------------------------|------| -------------- |
| system-service           | 8000 | 16gmaster  |
| data-market-service      | 8822 | 16gmaster  |
| service-data-integration | 8824 | 16gmaster  |
| data-metadata-service    | 8820 | 16gmaster  |
| data-system-service      | 8810 | 16gmaster  |
| service-data-dts         | 9536 | 16gmaster  |
| config                   | 8611 | 16gmaster  |

| 16gslave                      | port | ip             |
|-------------------------------| ---- | -------------- |
| eureka                  | 8610 | 16gslave    |
| service-workflow        | 8814 | 16gslave    |
| data-metadata-service-console    | 8821 | 16gslave    |
| service-data-mapping    | 8823 | 16gslave    |
| data-masterdata-service | 8828 | 16gslave    |
| data-quality-service    | 8826 | 16gslave    |

| 16gdata               | port | ip             |
|-----------------------| ---- | -------------- |
| data-standard-service | 8825 | 16gdata |
| data-visual-service   | 8827 | 16gdata |
| email-service         | 8812 | 16gdata |
| file-service          | 8811 | 16gdata |
| quartz-service        | 8813 | 16gdata |
| gateway               | 9538 | 16gslave    |


### 部署方式

> 数据库版本为 **mysql5.7** 及以上版本
### 1、`studio`数据库初始化
>
> 1.1 source install/sql/studio.sql
> 1.2 source install/sql/studio-v0.x.x.sql

### 2、修改 **config** 配置中心

> **config** 文件夹下的配置文件, 修改 **redis**, **mysql** 和 **rabbitmq** 的配置信息
>
### 3、项目根目录下执行
```
1、缺失aspose-words,要手动安装到本地仓库
2、cd alldata/studio/common
3、安装命令：windows使用git bash执行, mac直接执行以下命令
4、mvn install:install-file -Dfile=aspose-words-20.3.jar -DgroupId=com.aspose -DartifactId=aspose-words -Dversion=20.3 -Dpackaging=jar
5、安装成功重新刷新依赖,重新打包
```
> cd alldata/studio/common
> mvn install:install-file -Dfile=/alldata/studio/common/aspose-words-20.3.jar -DgroupId=com.aspose -DartifactId=aspose-words -Dversion=20.3 -Dpackaging=jar
> mvn clean install -DskipTests && mvn clean package -DskipTests
>
> 获取安装包build/studio-release-0.4.x.tar.gz
>
> 上传服务器解压
>
### 4、部署`stuido`[后端]
## 单节点启动[All In One]

> 1、启动eureka on `16gslave`
>
> 2、启动config on `16gmaster`
>
> 3、启动gateway on `16gdata`
>
> 4、启动其他Jar

## 三节点启动[16gmaster, 16gslave, 16gdata]
> 1. 单独启动 eureka on `16gslave`
>
> 2. 单独启动config on `16gmaster`
>
> 3. 单独启动gateway on `16gdata`
>
> 4. 启动`16gslave`, sh start16gslave.sh
>
> 5. 启动`16gdata`, sh start16gdata.sh
>
> 6. 启动`16gmaster`, sh start16gmaster.sh

### 5、部署`studio`[前端]:
## 前端部署

### 安装依赖

> 依次安装：
> nvm install v10.15.3 && nvm use v10.15.3

> npm install -g @vue/cli

> npm install script-loader

> npm install jsonlint

> npm install vue2-jsoneditor

> npm install

> npm run build:prod [生产]
>
> 生产环境启动前端ui项目, 需要[配置nginx]
```markdown
# For more information on configuration, see:
#   * Official English Documentation: http://nginx.org/en/docs/
#   * Official Russian Documentation: http://nginx.org/ru/docs/

user nginx;
worker_processes auto;
error_log /var/log/nginx/error.log;
pid /run/nginx.pid;

# Load dynamic modules. See /usr/share/doc/nginx/README.dynamic.
include /usr/share/nginx/modules/*.conf;

events {
worker_connections 1024;
}

http {
log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
'$status $body_bytes_sent "$http_referer" '
'"$http_user_agent" "$http_x_forwarded_for"';

    access_log  /var/log/nginx/access.log  main;

    sendfile            on;
    tcp_nopush          on;
    tcp_nodelay         on;
    keepalive_timeout   65;
    types_hash_max_size 4096;

    include             /etc/nginx/mime.types;
    default_type        application/octet-stream;

    # Load modular configuration files from the /etc/nginx/conf.d directory.
    # See http://nginx.org/en/docs/ngx_core_module.html#include
    # for more information.
    include /etc/nginx/conf.d/*.conf;
    server {
		listen       80;
		server_name  16gmaster;	
		add_header Access-Control-Allow-Origin *;
		add_header Access-Control-Allow-Headers X-Requested-With;
		add_header Access-Control-Allow-Methods GET,POST,OPTIONS;
		location / {
			root /studio/ui/dist;
			index index.html;
			try_files $uri $uri/ /index.html;
		}
		location /api/ {
			proxy_pass  http://16gdata:9538/;
			proxy_set_header Host $proxy_host;
			proxy_set_header X-Real-IP $remote_addr;
			proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
		}
	}
}
```
> 测试环境启动前端ui项目
>
> npm run dev [测试]
>
> 访问`studio`页面
>
> curl http://localhost:8013
>
> 用户名：admin 密码：123456

## 数据集成配置教程

> 先找到用户管理-菜单管理, 新增【数据集成】目录
>
> 新增【数据集成】下面的菜单, 菜单各项按如下配置输入, 之后进入角色管理
>
> 配置admin账号的目录数据权限, 选中刚才新增的数据集成目录及里面的菜单, 刷新或重新登录即可访问【数据集成】

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/9457212/233446739-41ea4501-bb09-4eb2-86de-21c168784564.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/9457212/233446763-cbb15105-b209-4b8f-b3f2-c41b5a607dd9.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/9457212/233447516-c952efd0-f8e2-4181-8608-1f513f9c0e93.png">
<br/>

## Antlr4 SQL POC

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227958337-611c8b86-bc99-4a42-ad56-49f99531dd39.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227958353-4ec1b51b-1514-4845-a441-4d689f5d6fd8.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227958367-9ae952f6-adf3-4bbc-8191-2619785ddb9f.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/227958381-1650deff-6fe1-4cb2-ba39-5af04bb3cc1f.png">
<br/>
1、Antlr4词法解析和语法解析
> 包括词法解析、语法解析、Antlr4的结果的处理

2、Antlr4执行阶段
> 1. 分为Lexer和Parser，实际上表示了两个不同的阶段：
>
> 2. 词法分析阶段：对应于Lexer定义的词法规则，解析结果为一个一个的Token；
     > 解析阶段：根据词法，构造出来一棵解析树或者语法树。
>
> 3. 词法解析和语法解析的调和
> 4. 首先，语法解析相对于词法解析，会产生更多的开销，所以，应该尽量将某些可能的处理在词法解析阶段完成，减少语法解析阶段的开销
> 5. 合并语言不关心的标记，例如，某些语言（例如js）不区分int、double，只有 number，那么在词法解析阶段，
     > 就不需要将int和double区分开，统一合并为一个number；
> 6. 空格、注释等信息，对于语法解析并无大的帮助，可以在词法分析阶段剔除掉；
     > 诸如标志符、关键字、字符串和数字这样的常用记号，均应该在词法解析时完成，而不要到语法解析阶段再进行。
> 7. 只有 number，没有 int 和 double 等，但是面向静态代码分析，我们可能需要知道确切的类型来帮助分析特定的缺陷；
>
> 8. 虽然注释对代码帮助不大，但是我们有时候也需要解析注释的内容来进行分析，如果无法在语法解析的时候获取，
     > 那么就需要遍历Token，从而导致静态代码分析开销更大等；
> 9. 解析树vs语法树
> 10. Antlr4生成的树状结构，称为解析树或者是语法树，如果通过Antlr4解析语言简单使用，可以直接基于Antlr4的结果开发，
      但是如果要进行更加深入的处理，就需要对Antlr4的结果进行更进一步的处理，以更符合我们的使用习惯
> 11. Java Parser格式的Java的AST，Clang格式的C/C++的AST, 然后才能更好地在上面进行开发。


## Presto POC调研
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/225311825-e066bf16-42ea-4995-9547-f76bdb5495b2.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/225311708-9f9b0227-8bd6-4e72-b290-1625b9bfba6e.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/225311709-9811642b-7c41-4c41-807e-eb517ebcbc5f.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/225311725-e348506b-840e-4ed3-94a5-c299c774d7f5.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/225311739-17f2aa9e-b72e-4076-8d5f-39c54122279b.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/225311752-f6423871-943e-475c-a300-d5da3504a25b.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/225311784-fbb18595-b0b0-469a-90c3-1474a5833d49.png">
<br/>

> 1. cd presto && mvn clean install -DskipTests=TRUE
> 2. cd presto-server/target/presto-server-0.280-SNAPSHOT/
> 3. tar -zxvf presto-server-0.280-SNAPSHOT.tar.gz
> 4. cd presto-server-0.280-SNAPSHOT/
> 5. client端访问：presto-cli/target
> 6. java -jar presto-cli-0.280-SNAPSHOT-executable.jar --server=localhost:8080 默认端口为8080
> 7. 测试SQL：SELECT * FROM system.runtime.nodes;
> 8. 网页访问：http://localhost:8080

## Griffin POC调研
> 安装apache-maven-3.6.3
>
> cd griffin && mvn clean package -DskipTests=TRUE
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/224027349-f9298f12-4ab5-4521-ab16-c81db8032576.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/224027365-417948b6-a948-43bb-9cba-d50456818dbf.png">
<br/>

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/171132004-542b699c-2878-4648-a79e-f118f28a0ed2.png">
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/171131705-86a2c0bd-cd9d-4a66-b209-5c41d1b18e56.png">
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/171132036-613e1271-d122-47dc-af7c-a3ee2a203a2e.png">
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/171132186-261b742a-dc88-4739-8327-08b503fce8d8.png">
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/171132633-193bbba4-58d6-4b38-8e9e-4674cdfa7cdd.png">
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/171132684-37ebcec6-05dd-45d6-83cd-d4f18416b755.png"> 
<br/>

<br/>

### Livy访问查看JOB

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/171131636-4cb6d93b-c994-4dfa-bfee-48d2a04c4963.png">  
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/171133364-8e4a8e84-c9f9-456c-9f33-c90b90cf54e4.png"> 
<br/>

## Calcite POC调研
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/223409318-f0154e9f-d247-4ecb-ab57-b6ea5bfe2881.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/223409331-044d0013-17fe-424f-9a47-dcb5b7d3267c.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/223409339-3d5205da-2a3c-46d0-a327-5f163ca5e079.png">
<br/>

### Calcite本地安装部署

> 1. 配置Gradle,在USER_HOME/.gradle/下创建init.gradle文件
>
> 2. init.gradle文件
```markdown
allprojects{
    repositories {
        def ALIYUN_REPOSITORY_URL = 'http://maven.aliyun.com/nexus/content/groups/public'
        def ALIYUN_JCENTER_URL = 'http://maven.aliyun.com/nexus/content/repositories/jcenter'
        def GRADLE_LOCAL_RELEASE_URL = 'https://repo.gradle.org/gradle/libs-releases-local'
        def ALIYUN_SPRING_RELEASE_URL = 'https://maven.aliyun.com/repository/spring-plugin'

        all { ArtifactRepository repo ->
            if(repo instanceof MavenArtifactRepository){
                def url = repo.url.toString()
                if (url.startsWith('https://repo1.maven.org/maven2')) {
                    project.logger.lifecycle "Repository ${repo.url} replaced by $ALIYUN_REPOSITORY_URL."
                    remove repo
                }
                if (url.startsWith('https://jcenter.bintray.com/')) {
                    project.logger.lifecycle "Repository ${repo.url} replaced by $ALIYUN_JCENTER_URL."
                    remove repo
                }
                if (url.startsWith('http://repo.spring.io/plugins-release')) {
                    project.logger.lifecycle "Repository ${repo.url} replaced by $ALIYUN_SPRING_RELEASE_URL."
                    remove repo
                }

            }
        }
        maven {
		    allowInsecureProtocol = true
            url ALIYUN_REPOSITORY_URL
        }

        maven {
            allowInsecureProtocol = true
            url ALIYUN_JCENTER_URL
        }
        maven {
            allowInsecureProtocol = true
            url ALIYUN_SPRING_RELEASE_URL
        }
        maven {
            allowInsecureProtocol = true
            url GRADLE_LOCAL_RELEASE_URL
        }

    }
}
```
> 3. ./gradlew build 如果跳过测试使用./gradlew build -x test
> 4. 构建成功获取构建包
> 5. In the release/build/distributions
     > apache-calcite-1.33.0-SNAPSHOT-src.tar.gz
     > apache-calcite-1.33.0-SNAPSHOT-src.tar.gz.sha512
> 6. tar -zxvf apache-calcite-1.33.0-SNAPSHOT-src.tar.gz
> 7. cd apache-calcite-1.33.0-SNAPSHOT-src
> 8. cd example/csv/ && cp -r /mnt/poc/alldatadc/calcite/calcite-1.33.0/build .
> 9. 安装配置gradle7.4.2, cd /opt/gradle
> 10. wget https://services.gradle.org/distributions/gradle-7.4.2-all.zip
> 11. 解压 unzip gradle-7.4.2-all.zip
> 12. 配置环境变量：export PATH=$PATH:/opt/gradle/gradle-7.4.2/bin
```markdown
[root@16gdata apache-calcite-1.33.0-SNAPSHOT-src]# cd example/csv/
[root@16gdata csv]# ll
total 24
drwxr-xr-x 10 root root 4096 Mar  7 18:45 build
-rw-rw-r--  1 root root 3577 Jan  2  1970 build.gradle.kts
-rw-rw-r--  1 root root  876 Jan  2  1970 gradle.properties
-rwxr-xr-x  1 root root 1793 Mar  7 18:44 sqlline
-rw-rw-r--  1 root root 1537 Jan  2  1970 sqlline.bat
drwxrwxr-x  4 root root 4096 Jan  2  1970 src
```
> 13. 运行./sqlline
> 14. 进入命令行测试sqlline
```markdown
[root@16gdata csv]# !connect jdbc:calcite:model=src/test/resources/model.json admin admin
-bash: !connect: event not found
[root@16gdata csv]# ./sqlline
Building Apache Calcite 1.33.0-SNAPSHOT
sqlline version 1.12.0
sqlline> !connect jdbc:calcite:model=src/test/resources/model.json admin admin
Transaction isolation level TRANSACTION_REPEATABLE_READ is not supported. Default (TRANSACTION_NONE) will be used instead.
0: jdbc:calcite:model=src/test/resources/mode> !tables
+-----------+-------------+------------+--------------+---------+----------+------------+-----------+---------------------------+----------------+
| TABLE_CAT | TABLE_SCHEM | TABLE_NAME |  TABLE_TYPE  | REMARKS | TYPE_CAT | TYPE_SCHEM | TYPE_NAME | SELF_REFERENCING_COL_NAME | REF_GENERATION |
+-----------+-------------+------------+--------------+---------+----------+------------+-----------+---------------------------+----------------+
|           | SALES       | DEPTS      | TABLE        |         |          |            |           |                           |                |
|           | SALES       | EMPS       | TABLE        |         |          |            |           |                           |                |
|           | SALES       | SDEPTS     | TABLE        |         |          |            |           |                           |                |
|           | metadata    | COLUMNS    | SYSTEM TABLE |         |          |            |           |                           |                |
|           | metadata    | TABLES     | SYSTEM TABLE |         |          |            |           |                           |                |
+-----------+-------------+------------+--------------+---------+----------+------------+-----------+---------------------------+----------------+
0: jdbc:calcite:model=src/test/resources/mode> select * from SALES.SDEPTS;
+--------+-----------+
| DEPTNO |   NAME    |
+--------+-----------+
| 10     | Sales     |
| 20     | Marketing |
| 30     | Accounts  |
| 40     | 40        |
| 50     | 50        |
| 60     | 60        |
+--------+-----------+
6 rows selected (1.336 seconds)
0: jdbc:calcite:model=src/test/resources/mode>

```


## Doris POC调研
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/222687597-0700e9b6-9c92-4138-a341-2a85b814e236.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/222687599-63f060c6-8859-45b4-b0fa-d7f5e1e6063b.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/222687609-0910d328-92e6-4849-b7a5-013dc47e514b.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/222687618-c76eebe1-253e-4a10-b047-9f3903809eed.png">
<br/>

> 参考https://doris.apache.org/zh-CN/docs/install/source-install/compilation
> 1. docker pull apache/doris:build-env-for-1.1.0
> 2. docker run -it -v /root/.m2:/root/.m2 -v /mnt/poc/alldatadc/doris/:/root/doris/ apache/doris:build-env-for-1.1.0
> 3. 进入docker
> 4. wget https://archive.apache.org/dist/doris/1.1/1.1.0-rc05/apache-doris-1.1.0-src.tar.gz
> 5. tar -zxvf apache-doris-1.1.0-src.tar.gz
> 6. mv apache-doris-1.1.0-src doris-1.1.0
> 7. sh build.sh
> 8. web访问http://16gdata:8080
> 9. 参考https://doris.apache.org/zh-CN/docs/get-starting/
> 10. 启动fe ./bin/start_fe.sh --daemon 成功启动：curl http://127.0.0.1:8030/api/bootstrap
> 11. mysql -uroot -P9030 -h127.0.0.1 然后ALTER SYSTEM ADD BACKEND "127.0.0.1:9050";
> 13. cp java-udf-jar-with-dependencies.jar ./be/lib/
> 14. 启动be ./bin/start_be.sh --daemon
> 15. 创建demo库表
```markdown
use demo;

CREATE TABLE IF NOT EXISTS demo.example_tbl
(
    `user_id` LARGEINT NOT NULL COMMENT "用户id",
    `date` DATE NOT NULL COMMENT "数据灌入日期时间",
    `city` VARCHAR(20) COMMENT "用户所在城市",
    `age` SMALLINT COMMENT "用户年龄",
    `sex` TINYINT COMMENT "用户性别",
    `last_visit_date` DATETIME REPLACE DEFAULT "1970-01-01 00:00:00" COMMENT "用户最后一次访问时间",
    `cost` BIGINT SUM DEFAULT "0" COMMENT "用户总消费",
    `max_dwell_time` INT MAX DEFAULT "0" COMMENT "用户最大停留时间",
    `min_dwell_time` INT MIN DEFAULT "99999" COMMENT "用户最小停留时间"
)
AGGREGATE KEY(`user_id`, `date`, `city`, `age`, `sex`)
DISTRIBUTED BY HASH(`user_id`) BUCKETS 1
PROPERTIES (
    "replication_allocation" = "tag.location.default: 1"
);
```
> 16. touch sample.csv
```markdown
10000,2017-10-01,北京,20,0,2017-10-01 06:00:00,20,10,10
10000,2017-10-01,北京,20,0,2017-10-01 07:00:00,15,2,2
10001,2017-10-01,北京,30,1,2017-10-01 17:05:45,2,22,22
10002,2017-10-02,上海,20,1,2017-10-02 12:59:12,200,5,5
10003,2017-10-02,广州,32,0,2017-10-02 11:20:00,30,11,11
10004,2017-10-01,深圳,35,0,2017-10-01 10:00:15,100,3,3
10004,2017-10-03,深圳,35,0,2017-10-03 10:20:22,11,6,6
```
> 17. curl  --location-trusted -u root: -T sample.csv -H "column_separator:," http://127.0.0.1:8030/api/demo/example_tbl/_stream_load
> 18. 访问http://127.0.0.1:8030/ 账密：root/空密码 输入sql查询： select * from demo.example_tbl

## 本地启动TIS POC调研
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/222085646-6330c77e-3aeb-4d8b-b46d-7c8643b743c1.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/222085660-dd395d6a-2c02-42dc-9d60-6b1befdc795f.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/222085664-ce6928e8-0c2d-4c0d-842e-c74af6cdeaa7.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/222085671-6eb2bc96-5cf0-4069-b6d9-be5a2592bae4.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/222085683-b1f0f4bd-2fec-4019-8d91-1a5fbb0c721e.png">
<br/>

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/221879155-682c5fec-3c61-4a91-aaf4-e55cc8b1b2c3.png">
<br/>

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/221879168-2f221f66-f43a-455a-b5f8-b762f2dae124.png">
<br/>

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/221879179-76eb538c-8d59-4ebb-ac45-05fb9b1f088f.png">
<br/>

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/221879200-240e03f8-0e0e-4fc2-93d6-9d8e23ef8eb3.png">
<br/>

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/221879216-81039329-a244-4ff4-ae1c-1d4815a4d5d4.png">
<br/>

> 1. 部署TIS-参考tis/README.md

> https://tis.pub/docs/develop/compile-running
## 本地启动部署Tis
> 1. mvn clean install -Dmaven.test.skip=true
>
> 2. 配置数据库
>
> source /tis-ansible/tis_console_mysql.sql
>
> 3. 配置项目web
>
> vi /tis/tis-web-config/config.properties
```markdown
project.name=TIS
runtime=daily

tis.datasource.type=mysql
tis.datasource.url=16gmaster
tis.datasource.port=3306
tis.datasource.username=root
tis.datasource.password=123456
tis.datasource.dbname=tis_console
zk.host=16gmaster:2181/tis/cloud

assemble.host=8gmaster
tis.host=8gmaster

```
> 4. 启动TIS
>
> mvn compile test -Dtest=StartTISWeb Dtis.launch.port=8080
>
> 访问 http://8gmaster:8080

> 2. 部署plugins,参考plugins/README.md

## 本地安装部署 on Linux
> 1. 安装maven3.8.1 配置settings.xml
>
> 2. Only配置
>
```
     <mirror>
      <id>alimaven</id>
      <name>aliyun maven</name>
      <url>http://maven.aliyun.com/nexus/content/groups/public/</url>
      <mirrorOf>central</mirrorOf>
    </mirror>

```
> 3. 创建目录 /opt/data/tis/libs/plugins
>
> 4. 执行plugin软连接配置
>
```
for f in `find /mnt/poc/alldatadc/tis_poc/plugins  -name '*.tpi' -print` do echo " ln -s $f " ln -s $f /opt/data/tis/libs/plugins/${f##*/} done ;
```
> 5. 安装plugins
>
> mvn clean package -Dmaven.test.skip=true -Dappname=all
>

> 3. 部署ng-tis,参考ng-tis/README.md
## 本地打包部署 on Linux
> 1. nvm install v10.15.3
>
> 2. npm install -g @angular/cli@12.2.13
>
> 3. npm run ng:serve-jit --scripts-prepend-node-path=auto
>
> 4. curl http://localhost:4200
>
## DataVines 数据质量POC调研
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/221765941-b2903701-d4f3-4895-9f1d-7f317998b9ae.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/221765954-dfc3148e-73f9-4ed1-a39a-05aa97a346b5.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/221765962-ede68668-56a6-4b80-bc17-950b2b48aa8e.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/221765970-72cb3a27-e06d-4a54-a274-5b1a31f9bbd2.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/221765976-c9c9e116-2047-46dc-901c-2bf89a615d73.png">
<br/>

## DataHub POC调研
### 本地开发、构建、启动 On Linux
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/221186552-50bf2644-3ce6-4a22-944d-bfd7ab0d91f0.png">
<br/>
### DataHub源码构建
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/221185131-5420b956-4ffb-4041-a286-166f02907954.png">
<br/>
### 命令行安装成功
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/221185319-3f0046bf-5ed3-4602-a615-3ca638bbcf0a.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/221185417-4a095557-80ef-4c37-b32c-203c4b06c53f.png">
<br/>

### DataHub架构
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/221185460-caa019b9-04f3-4b64-9550-b6d192075a37.png">
<br/>

> DataHub (& GMA) 架构
> DatahHub 采用前后端分离 + 微服务 / 容器架构
> 前端：Ember + TypeScript + ES9 + ES.Next + Yarn + ESLint
>
> 服务端：Play Framework（web 框架） + Spring + Rest.li（restful 框架）+ Pegasus（数据建模语言） + Apache Samza （流处理框架）
>
> 基础设施：elastic search (5.6) + Mysql + neo4j + kafka
>
> 构建工具：Gradlew + Docker + Docker compose
>
> DataHub 组成
>
> datahub-gms (Generalized Metadata Store) ： 元数据存储服务
>
> datahub-gma (Generalized Metadata Architecture) ： 通用元数据体系结构
>
> GMA 是 datahub 的基础设施，提供标准化的元数据模型和访问层
>
> datahub-frontend ： 应用前端
>
> datahub-mxe 元数据事件datahub-mce-consumer （MetadataChangeEvent）：元数据变更事件，由平台或爬虫程序发起，写入到 GMS
>
> datahub-mae-consumer (MetadataAuditEvent)： 元数据审计事件，只有被成功处理的 MCE 才会产生相应的 MAE，由 GMS 发起 ，写入到 es&Neo4j

### 1、JAVA_HOME
> 1.1 安装Java-11 && 配置JAVA_HOME
>
> sudo yum install java-11-openjdk -y
>
> sudo yum install java-11-openjdk-devel
>
> 1.2 安装Java-8 && 不需要配置JAVA_HOME
>
> yum install java-1.8.0-openjdk.x86_64
>
> yum install -y java-1.8.0-openjdk-devel.x86_64

### 2、Python3.7以上版本
> 2.1 下载python3.7
>
> mkdir -p /usr/local/python3 && cd /usr/local/python3
>
> wget https://www.python.org/ftp/python/3.7.16/Python-3.7.16.tar.xz
>
> tar -xvf Python-3.7.16.tar.xz
>
> cd Python-3.7.16
>
> ./configure --prefix=/usr/local/python3
>
> make && make install
>
> ln -s /usr/local/python3/bin/python3 /usr/bin/python3
>
> 验证python3.7版本

### 3、源码构建
> 3.1 安装sasl、fastjsonschema
>
> 3.1.1 yum -y install cyrus-sasl cyrus-sasl-devel cyrus-sasl-lib
>
> 3.1.2 pip3 install fastjsonschema
>
> 3.1.3 yum -y install openldap-devel
>
> 3.1.4 pip3 install python_ldap
>
> 3.1.5 cd cd smoke-test && pip install -r requirements.txt
>
> 3.2 安装命令行
>
> ./gradlew :metadata-ingestion:installDev
>
> 3.3 后端打包
>
> 执行./gradlew metadata-service:war:build
>
> 3.4 前端打包
>
> 修改node版本: 找到datahub-0.10.0/datahub-web-react/build.gradle, 修改version为'16.10.0'
>
> export NODE_OPTIONS="--max-old-space-size=8192"
>
> 执行./gradlew :datahub-frontend:dist -x yarnTest -x yarnLint

### 4 启动datahub
> 新增docker-compose.yml
```
networks:
  default:
    name: datahub_network
services:
  broker:
    container_name: broker
    depends_on:
      - zookeeper
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
      - KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS=0
      - KAFKA_HEAP_OPTS=-Xms256m -Xmx256m
      - KAFKA_CONFLUENT_SUPPORT_METRICS_ENABLE=false
    hostname: broker
    image: confluentinc/cp-kafka:7.2.2
    ports:
      - ${DATAHUB_MAPPED_KAFKA_BROKER_PORT:-9092}:9092
  datahub-actions:
    depends_on:
      - datahub-gms
    environment:
      - DATAHUB_GMS_HOST=datahub-gms
      - DATAHUB_GMS_PORT=8080
      - DATAHUB_GMS_PROTOCOL=http
      - DATAHUB_SYSTEM_CLIENT_ID=__datahub_system
      - DATAHUB_SYSTEM_CLIENT_SECRET=JohnSnowKnowsNothing
      - KAFKA_BOOTSTRAP_SERVER=broker:29092
      - KAFKA_PROPERTIES_SECURITY_PROTOCOL=PLAINTEXT
      - METADATA_AUDIT_EVENT_NAME=MetadataAuditEvent_v4
      - METADATA_CHANGE_LOG_VERSIONED_TOPIC_NAME=MetadataChangeLog_Versioned_v1
      - SCHEMA_REGISTRY_URL=http://schema-registry:8081
    hostname: actions
    image: acryldata/datahub-actions:${ACTIONS_VERSION:-head}
    restart: on-failure:5
  datahub-frontend-react:
    container_name: datahub-frontend-react
    depends_on:
      - datahub-gms
    environment:
      - DATAHUB_GMS_HOST=datahub-gms
      - DATAHUB_GMS_PORT=8080
      - DATAHUB_SECRET=YouKnowNothing
      - DATAHUB_APP_VERSION=1.0
      - DATAHUB_PLAY_MEM_BUFFER_SIZE=10MB
      - JAVA_OPTS=-Xms512m -Xmx512m -Dhttp.port=9002 -Dconfig.file=datahub-frontend/conf/application.conf -Djava.security.auth.login.config=datahub-frontend/conf/jaas.conf -Dlogback.configurationFile=datahub-frontend/conf/logback.xml -Dlogback.debug=false -Dpidfile.path=/dev/null
      - KAFKA_BOOTSTRAP_SERVER=broker:29092
      - DATAHUB_TRACKING_TOPIC=DataHubUsageEvent_v1
      - ELASTIC_CLIENT_HOST=elasticsearch
      - ELASTIC_CLIENT_PORT=9200
    hostname: datahub-frontend-react
    image: ${DATAHUB_FRONTEND_IMAGE:-linkedin/datahub-frontend-react}:${DATAHUB_VERSION:-head}
    ports:
      - ${DATAHUB_MAPPED_FRONTEND_PORT:-9002}:9002
    volumes:
      - ${HOME}/.datahub/plugins:/etc/datahub/plugins
  datahub-gms:
    container_name: datahub-gms
    depends_on:
      - mysql
    environment:
      - DATAHUB_SERVER_TYPE=${DATAHUB_SERVER_TYPE:-quickstart}
      - DATAHUB_TELEMETRY_ENABLED=${DATAHUB_TELEMETRY_ENABLED:-true}
      - DATAHUB_UPGRADE_HISTORY_KAFKA_CONSUMER_GROUP_ID=generic-duhe-consumer-job-client-gms
      - EBEAN_DATASOURCE_DRIVER=com.mysql.jdbc.Driver
      - EBEAN_DATASOURCE_HOST=mysql:3306
      - EBEAN_DATASOURCE_PASSWORD=datahub
      - EBEAN_DATASOURCE_URL=jdbc:mysql://mysql:3306/datahub?verifyServerCertificate=false&useSSL=false&useUnicode=yes&characterEncoding=UTF-8
      - EBEAN_DATASOURCE_USERNAME=datahub
      - ELASTICSEARCH_HOST=elasticsearch
      - ELASTICSEARCH_INDEX_BUILDER_MAPPINGS_REINDEX=true
      - ELASTICSEARCH_INDEX_BUILDER_SETTINGS_REINDEX=true
      - ELASTICSEARCH_PORT=9200
      - ENTITY_REGISTRY_CONFIG_PATH=/datahub/datahub-gms/resources/entity-registry.yml
      - ENTITY_SERVICE_ENABLE_RETENTION=true
      - ES_BULK_REFRESH_POLICY=WAIT_UNTIL
      - GRAPH_SERVICE_DIFF_MODE_ENABLED=true
      - GRAPH_SERVICE_IMPL=elasticsearch
      - JAVA_OPTS=-Xms1g -Xmx1g
      - KAFKA_BOOTSTRAP_SERVER=broker:29092
      - KAFKA_SCHEMAREGISTRY_URL=http://schema-registry:8081
      - MAE_CONSUMER_ENABLED=true
      - MCE_CONSUMER_ENABLED=true
      - PE_CONSUMER_ENABLED=true
      - UI_INGESTION_ENABLED=true
    hostname: datahub-gms
    image: ${DATAHUB_GMS_IMAGE:-linkedin/datahub-gms}:${DATAHUB_VERSION:-head}
    ports:
      - ${DATAHUB_MAPPED_GMS_PORT:-8080}:8080
    volumes:
      - ${HOME}/.datahub/plugins:/etc/datahub/plugins
  datahub-upgrade:
    command:
      - -u
      - SystemUpdate
    container_name: datahub-upgrade
    environment:
      - EBEAN_DATASOURCE_USERNAME=datahub
      - EBEAN_DATASOURCE_PASSWORD=datahub
      - EBEAN_DATASOURCE_HOST=mysql:3306
      - EBEAN_DATASOURCE_URL=jdbc:mysql://mysql:3306/datahub?verifyServerCertificate=false&useSSL=false&useUnicode=yes&characterEncoding=UTF-8
      - EBEAN_DATASOURCE_DRIVER=com.mysql.jdbc.Driver
      - KAFKA_BOOTSTRAP_SERVER=broker:29092
      - KAFKA_SCHEMAREGISTRY_URL=http://schema-registry:8081
      - ELASTICSEARCH_HOST=elasticsearch
      - ELASTICSEARCH_PORT=9200
      - ELASTICSEARCH_INDEX_BUILDER_MAPPINGS_REINDEX=true
      - ELASTICSEARCH_INDEX_BUILDER_SETTINGS_REINDEX=true
      - ELASTICSEARCH_BUILD_INDICES_CLONE_INDICES=false
      - GRAPH_SERVICE_IMPL=elasticsearch
      - DATAHUB_GMS_HOST=datahub-gms
      - DATAHUB_GMS_PORT=8080
      - ENTITY_REGISTRY_CONFIG_PATH=/datahub/datahub-gms/resources/entity-registry.yml
    hostname: datahub-upgrade
    image: ${DATAHUB_UPGRADE_IMAGE:-acryldata/datahub-upgrade}:${DATAHUB_VERSION:-head}
  elasticsearch:
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - ES_JAVA_OPTS=-Xms256m -Xmx512m -Dlog4j2.formatMsgNoLookups=true
    healthcheck:
      retries: 4
      start_period: 2m
      test:
        - CMD-SHELL
        - curl -sS --fail 'http://localhost:9200/_cluster/health?wait_for_status=yellow&timeout=0s' || exit 1
    hostname: elasticsearch
    image: elasticsearch:7.10.1
    mem_limit: 1g
    ports:
      - ${DATAHUB_MAPPED_ELASTIC_PORT:-9200}:9200
    volumes:
      - esdata:/usr/share/elasticsearch/data
  elasticsearch-setup:
    container_name: elasticsearch-setup
    depends_on:
      - elasticsearch
    environment:
      - ELASTICSEARCH_HOST=elasticsearch
      - ELASTICSEARCH_PORT=9200
      - ELASTICSEARCH_PROTOCOL=http
    hostname: elasticsearch-setup
    image: ${DATAHUB_ELASTIC_SETUP_IMAGE:-linkedin/datahub-elasticsearch-setup}:${DATAHUB_VERSION:-head}
  kafka-setup:
    container_name: kafka-setup
    depends_on:
      - broker
      - schema-registry
    environment:
      - DATAHUB_PRECREATE_TOPICS=${DATAHUB_PRECREATE_TOPICS:-false}
      - KAFKA_BOOTSTRAP_SERVER=broker:29092
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
    hostname: kafka-setup
    image: ${DATAHUB_KAFKA_SETUP_IMAGE:-linkedin/datahub-kafka-setup}:${DATAHUB_VERSION:-head}
  mysql:
    command: --character-set-server=utf8mb4 --collation-server=utf8mb4_bin --default-authentication-plugin=mysql_native_password
    container_name: mysql
    environment:
      - MYSQL_DATABASE=datahub
      - MYSQL_USER=datahub
      - MYSQL_PASSWORD=datahub
      - MYSQL_ROOT_PASSWORD=datahub
    hostname: mysql
    image: mysql:5.7
    ports:
      - ${DATAHUB_MAPPED_MYSQL_PORT:-33061}:3306
    volumes:
      - ../mysql/init.sql:/docker-entrypoint-initdb.d/init.sql
      - mysqldata:/var/lib/mysql
  mysql-setup:
    container_name: mysql-setup
    depends_on:
      - mysql
    environment:
      - MYSQL_HOST=mysql
      - MYSQL_PORT=3306
      - MYSQL_USERNAME=datahub
      - MYSQL_PASSWORD=datahub
      - DATAHUB_DB_NAME=datahub
    hostname: mysql-setup
    image: ${DATAHUB_MYSQL_SETUP_IMAGE:-acryldata/datahub-mysql-setup}:${DATAHUB_VERSION:-head}
  schema-registry:
    container_name: schema-registry
    depends_on:
      - broker
    environment:
      - SCHEMA_REGISTRY_HOST_NAME=schemaregistry
      - SCHEMA_REGISTRY_KAFKASTORE_SECURITY_PROTOCOL=PLAINTEXT
      - SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS=broker:29092
    hostname: schema-registry
    image: confluentinc/cp-schema-registry:7.2.2
    ports:
      - ${DATAHUB_MAPPED_SCHEMA_REGISTRY_PORT:-8081}:8081
  zookeeper:
    container_name: zookeeper
    environment:
      - ZOOKEEPER_CLIENT_PORT=2181
      - ZOOKEEPER_TICK_TIME=2000
    hostname: zookeeper
    image: confluentinc/cp-zookeeper:7.2.2
    ports:
      - ${DATAHUB_MAPPED_ZK_PORT:-2181}:2181
    volumes:
      - zkdata:/var/lib/zookeeper
version: "2.3"
volumes:
  esdata: null
  mysqldata: null
  zkdata: null
```
> python3 -m datahub docker quickstart --start -f docker-compose.yml

### 5 停止datahub
> python3 -m datahub docker quickstart --stop -f docker-compose.yml

##  dinky新增hive2flink任务类型

### 1, 支持执行提交hive sql running on flink

### 2, 测试代码
```
@Test
void testCreateDatabase() {
    sql("create database db1").ok("CREATE DATABASE `DB1`");
    sql("create database db1 comment 'comment db1' location '/path/to/db1'")
            .ok(
                    "CREATE DATABASE `DB1`\n"
                            + "COMMENT 'comment db1'\n"
                            + "LOCATION '/path/to/db1'");
    sql("create database db1 with dbproperties ('k1'='v1','k2'='v2')")
            .ok(
                    "CREATE DATABASE `DB1` WITH DBPROPERTIES (\n"
                            + "  'k1' = 'v1',\n"
                            + "  'k2' = 'v2'\n"
                            + ")");
}
```

### 3, 结果预览

> 测试FlinkHiveSqlParser Passed

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/216624287-ecd671ab-33f0-4ce8-a938-2e4576e21e2b.png">
<br/>

## Flink数据血缘初体验

### 1 结果预览
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/206093771-adfeebf0-ff7d-4044-b3b5-592ff965afa4.png">
<br/>

### 2 创建FlinkDDL

> 参考Resource/FlinkDDLSQL.sql

> CREATE TABLE data_gen (
>
> amount BIGINT
>
> ) WITH (
>
> 'connector' = 'datagen'
>
> 'rows-per-second' = '1'
>
> 'number-of-rows' = '3'
>
> 'fields.amount.kind' = 'random'
>
> 'fields.amount.min' = '10'
>
> 'fields.amount.max' = '11');
>
> CREATE TABLE mysql_sink (
>
> amount BIGINT
>
> PRIMARY KEY (amount) NOT ENFORCED
>
> ) WITH (
>
> 'connector' = 'jdbc'
>
> 'url' = 'jdbc:mysql://localhost:3306/test_db'
>
> 'table-name' = 'test_table'
>
> 'username' = 'root'
>
> 'password' = '123456'
>
> 'lookup.cache.max-rows' = '5000'
>
> 'lookup.cache.ttl' = '10min'
>
> );
>
> INSERT INTO mysql_sink SELECT amount as amount FROM data_gen;

### 3 执行com.platform.FlinkLineageBuild

> 获取结果

> 1, Flink血缘构建结果-表:
>
> [LineageTable{id='4', name='data_gen', columns=[LineageColumn{name='amount', title='amount'}]}
>
> LineageTable{id='6', name='mysql_sink', columns=[LineageColumn{name='amount', title='amount'}]}]
>
> 表ID: 4
>
> 表Namedata_gen
>
> 表ID: 4
>
> 表Namedata_gen
>
> 表-列LineageColumn{name='amount', title='amount'}
>
> 表ID: 6
>
> 表Namemysql_sink
>
> 表ID: 6
>
> 表Namemysql_sink
>
> 表-列LineageColumn{name='amount', title='amount'}
>
> 2, Flink血缘构建结果-边:
>
> [LineageRelation{id='1', srcTableId='4', tgtTableId='6', srcTableColName='amount', tgtTableColName='amount'}]
>
> 表-边: LineageRelation{id='1', srcTableId='4', tgtTableId='6', srcTableColName='amount', tgtTableColName='amount'}

## AllData Doris
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/200178943-7a8edb50-b4d6-4095-9e39-2b3303925701.png">
<br/>

## AllData全新定制一站式场景化大数据中台
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/200179453-bfe6b877-5e59-4239-8217-154a0953c97d.png">
<br/>

## 大数据组件管理DOCKER FOR DATA PLATFORM

### 1, 配置主机服务HOST

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203997670-4b2339e5-d4ba-43ec-afb3-e454646255fd.png">
<br/> 

### 2, 启动大数据集群

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203997598-a80fc4bc-1226-4d7a-9918-39f2586b4170.png">
<br/> 

### 3, YARN正常访问

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203997781-cb66da01-eddc-4576-b6e0-b107cdaa189b.png">
<br/>

### 4, HIVE正常使用

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203997829-52c8c396-1dc4-4a53-b398-c3a54685f66f.png">
<br/>

### 5, HDFS正常访问

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203998038-0cc73461-6fea-4779-b6c4-0ee293b62832.png">
<br/>

### 6, ES健康检测
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203998116-c9cf7a9a-c51e-48d3-823b-9ffef1806567.png">
<br/>

### 7, KIBANA UI访问

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203998431-e8c2a604-d666-4eef-9ea6-b8a2cc669cac.png">
<br/>

### 8, PRESTO UI访问

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203998248-bf6a4090-8943-493b-83a5-87a55d2dcc17.png">
<br/>

### 9, HBASE正常访问

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203998514-354d2756-0bee-4cbe-aead-d686fec61da4.png">
<br/>

### 10, FLIKN RUNTIME WEB 正常访问
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203998796-732df150-c697-41b6-a5e7-dc9c65cada4f.png">
<br/>

## 使用Docker/K8S云原生方案-控制各种组件起停

>
> 1, BUSINESS FOR ALL DATA PLATFORM 商业项目
>
> 2, BUSINESS FOR ALL DATA PLATFORM 计算引擎
>
> 3, DEVOPS FOR ALL DATA PLATFORM 运维引擎
>
> 4, DATA GOVERN FOR ALL DATA PLATFORM 数据治理引擎
>
> 5, DATA Integrate FOR ALL DATA PLATFORM 数据集成引擎
>
> 6, AI FOR ALL DATA PLATFORM 人工智能引擎
>
> 7, DATA ODS FOR ALL DATA PLATFORM 数据采集引擎
>
> 8, OLAP FOR ALL DATA PLATFORM OLAP查询引擎
>
> 9, OPTIMIZE FOR ALL DATA PLATFORM 性能优化引擎
>
> 10, DATABASES FOR ALL DATA PLATFORM 分布式存储引擎
>

## DataSophon POC
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/221186382-38f3b419-e710-4b1d-8af9-b176a6d565ac.png">
<br/>

### 一, 项目地址

> https://github.com/datasophon/datasophon
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/220853673-54092c18-3349-4bcc-b7c7-8b69d5107694.png">
<br/>

### 二, 官方文档

### https://datasophon.github.io/datasophon-website/docs/current/%E6%A6%82%E8%A7%88

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/220853686-3099ef43-587e-4a64-881a-4e7aceff6441.png">
<br/>

### 三, DataSophon+安装包

#### 提供了一系列包, 可以集成自定义服务
>
> DataSophon+tar.gz安装包, 直接替换Ambari+HDP, 解决大数据集群纳管组件的生命周期问题

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/220853785-595b760b-5019-4d4f-a6f8-efb5763a49e6.png">
<br/>

### 四, 架构流程图

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/220853807-1a45f3de-7dba-42fb-80ad-cb729b2ca5ef.png">
<br/>

### 五, 组件自定义服务配置

#### https://datasophon.github.io/datasophon-website/docs/current/%E5%BC%80%E5%8F%91%E8%80%85%E6%8C%87%E5%8D%97/%E7%BB%84%E4%BB%B6%E9%9B%86%E6%88%90/commit_code

### 六, 三件套做监控

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/220853843-908bd9bd-a716-4ce4-bff1-2fe37956671a.png">
<br/>

### 七, 未来规划

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/220853889-64ece11e-2c8d-47a7-9c65-8decd4e47635.png">
<br/>

#### 当前版本v1.1.0
>
> 支持系统租户管理
>
> 主机管理支持机架管理
>
> YARN资源调度支持容量调度器
>
> YARN资源调度支持标签调度
>
> 支持组件集成Kerberos, 可自由开启和关闭kerberos认证集成

### 八, 局限

> 版本与代码没有分离, 安装版本的hadoop-3.3.3

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/220853935-4ed97fe4-1b26-4f56-8a77-7229f91eb850.png">
<br/> 

### 九, 下载, 安装, 解压

> 丝滑切换Ambari Python install, status, start, stop

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/220853952-a93af1e0-6ac6-4222-8fa3-2b2446fe67ea.png">
<br/> 

### 十, 生态集成

> Dinky, Streampark, Doris等

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/220853996-bc592a28-c86f-4bdb-91af-26dcdbf835c7.png">
<br/>

### 十一, 启动逻辑

> 启动DS直接就执行对应的program

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/220854013-7d3700d2-f3f9-4ef7-9871-dcc078c69ce7.png">
<br/> 



### 十二, 自定义属性

> 自定义模板+freemaker, 应用自定义属性

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/220854025-474a5043-5063-4045-8f1c-9f6e999471e5.png">
<br/> 


### 十三, Actor监听消息通信

> Worker启动并监听Actor失败告警

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/220854046-7a2e1875-ecdc-4de4-be7c-5f1d22ffb319.png">
<br/> 


## Flink Table Store && Lake Storage POC

### 2.1 SQL~Flink table store poc
>
> set execution.checkpointing.interval=15sec;
>
> CREATE CATALOG alldata_catalog WITH (
>
>   'type'='table-store'
>
>   'warehouse'='file:/tmp/table_store'
>
> );
>
> USE CATALOG alldata_catalog;
>
> CREATE TABLE word_count (
>
>     word STRING PRIMARY KEY NOT ENFORCED
>     
>     cnt BIGINT
>
> );
>
> CREATE TEMPORARY TABLE word_table (
>
>     word STRING
>
> ) WITH (
>
>     'connector' = 'datagen'
>     
>     'fields.word.length' = '1'
>
> );
>
> INSERT INTO word_count SELECT word, COUNT(*) FROM word_table GROUP BY word;
>
> -- POC Test OLAP QUERY
>
> SET sql-client.execution.result-mode = 'tableau';
>
> RESET execution.checkpointing.interval;
>
> SET execution.runtime-mode = 'batch';
>
> SELECT * FROM word_count;
>
> -- POC Test Stream QUERY
>
> -- SET execution.runtime-mode = 'streaming';
>
> -- SELECT `interval`, COUNT(*) AS interval_cnt FROM
>
> --   (SELECT cnt / 10000 AS `interval` FROM word_count) GROUP BY `interval`;

### 2.2 Flink Runtime Web
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203073679-f64b4655-7ea8-4c36-98ab-7b1806119224.png">
<br/>

### 2.3 Flink Batch
>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203073715-e69d8378-1b37-4fea-851f-9f3e6a9d62eb.png">
<br/>

### 2.4 Flink Olap Read
>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203073740-e088e842-3010-42af-bfc2-0808d5e1940f.png">
<br/> 

### 2.5 Flink Stream Read
>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203073760-906f0b1c-498b-4713-931b-25a90f53e985.png">
<br/> 

## Dlink二开新增Flink1.16.0支持
### 1, Dlink配置Flink Table Store相关依赖
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203467342-fd24f652-2fb5-4e4e-9b6e-23a113817b6b.png">
<br/> 
### 2, Dlink启动并运行成功
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203467422-a2c39226-31c6-4998-a926-71068b36de4d.png">
<br/> 
### 3, OLAP查询
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203467379-b498da92-218f-4f29-a977-dd17fa374ea0.png">
<br/> 

### 4, Flink1.16.0 Dlink流式读
> 4.1 Stream Read 1
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203467499-e1541c84-f8c8-40ff-aa33-cdd35cae2932.png">
<br/>
> 4.2 Stream Read 2
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203467519-83fd40d5-823d-45b0-8cdd-09e0b9b09cb2.png">
<br/>


## Architecture
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/171598215-0914f665-9950-476c-97ff-e7e07aa10eaf.png">
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/171598333-d14ff53f-3af3-481c-9f60-4f891a535b5c.png">
<br/>

| Component                                                                       | Description                                                    | Important Composition       |
|---------------------------------------------------------------------------------|----------------------------------------------------------------|-----------------------------|
| [**ai**](https://github.com/alldatacenter/alldata/tree/master/ai)               | AI STUDIO FOR ALL DATA PLATFORM artificial intelligence engine | 人工智能引擎                      |
| [**dts**](https://github.com/alldatacenter/alldata/tree/master/dts)             | DTS FOR ALL DATA PLATFORM DATA DTS engine                      | 数据集成引擎                      |
| [**fs**](https://github.com/alldatacenter/alldata/tree/master/fs)               | DATA STORAGE FOR ALL DATA PLATFORM DATA STORAGE engine         | 大数据存储引擎                     |
| [**govern**](https://github.com/alldatacenter/alldata/tree/master/govern)       | DATA GOVERN FOR ALL DATA PLATFORM Data Governance Engine       | 数据治理引擎                      |
| [**iot**](https://github.com/alldatacenter/alldata/tree/master/iot)             | IOT FOR ALL DATA PLATFORM Data Governance Engine               | 云原生IOT开发框架                  |
| [**k8s**](https://github.com/alldatacenter/alldata/tree/master/k8s)               | Koordinator FOR ALL DATA PLATFORM Data Task Engine         | 知识图谱引擎                      |
| [**kg**](https://github.com/alldatacenter/alldata/tree/master/kg)               | KNOWLEDGE GRAPH FOR ALL DATA PLATFORM Data Task Engine         | 知识图谱引擎                      |
| [**lake**](https://github.com/alldatacenter/alldata/tree/master/lake) | LAKEHOUSE FOR ALL DATA PLATFORM ONE LAKE engine                 | 数据湖引擎                       |
| [**olap**](https://github.com/alldatacenter/alldata/tree/master/olap)           | OLAP FOR ALL DATA PLATFORM OLAP query engine                   | 混合OLAP查询引擎                  |
| [**shuffle**](https://github.com/alldatacenter/alldata/tree/master/shuffle)         | SHUFFLE FOR ALL DATA PLATFORM DATA SHUFFLE engine                  | Shuffule引擎                      |
| [**studio**](https://github.com/alldatacenter/alldata/tree/master/studio)         | STUDIO FOR ALL DATA PLATFORM DATA STUDIO engine                  | 数据工厂引擎                      |
| [**trade**](https://github.com/alldatacenter/alldata/tree/master/trade)         | TRADE FOR ALL DATA PLATFORM TRADE Engine                       | TRADE引擎                     |
| [**wiki**](https://github.com/alldatacenter/alldata/tree/master/wiki)           | WIKI FOR ALL DATA PLATFORM WIKI Engine                         | AllData知识库                  |
| [**alldata**](https://github.com/alldatacenter/all-in-data)                         | AllData社区项目通过二开大数据生态组件, 以及大数据采集, 大数据存储, 大数据计算, 大数据开发来建设一站式细分领域数字化解决方案    | Github一站式细分领域AllData数字化解决方案社区项目 |


## AllData社区商业计划图
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/188898972-d78bcbb6-eb30-420d-b5e1-7168aa340555.png">
<br/>

## AllData社区项目业务流程图
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/188899006-aba25703-f8fa-42b6-b59f-2573ee2b27fc.png">
<br/>

## AllData社区项目树状图
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/188898939-bfba6cbc-c7b0-40c4-becd-27152d5daa90.png">
<br/>

## 全站式AllData产品路线图
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/179927878-ff9c487e-0d30-49d5-bc88-6482646d90a8.png">
<br/>


## AllData社区项目时间旅行
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/188899033-948583a4-841b-4233-ad61-bbc45c936ca1.png">
<br/>

## 实时推荐系统业务流程图
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/191244864-3cffc8e3-e41e-4865-8b2b-376742f10a8e.png">
<br/>

## AllData总部前后端解决方案
### 包括AllData前后端解决方案, 多租户运维平台前后端
### 基于`eladmin` + `tenant` 建设AllData前后端解决方案

> 1, AllData前端解决方案 `studio/eladmin-web`
>
> 2, AllData后端解决方案 `studio/eladmin`
>
> 3, 多租户运维平台前端 `studio/tenant`
>
> 4, 多租户运维平台前端 `studio/tenantBack`

<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/196594418-1ba618cb-da53-487a-951d-0715e3fc685e.jpg">


## Integration

## 离线商城数仓展示
<br>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/160219586-e2e190fa-21f6-4f87-bbbc-7cdd6ecc625a.png">
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/160221446-24d9438d-703c-4d17-880e-5d34d0f8d229.png">
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/160221463-772477c8-f996-45df-ab74-9e7a179adc81.png">
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/160220078-bdabde8b-9467-4d26-8675-37712e1d48b1.png">
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/160220090-d5c33c1f-9507-4338-98e1-0abc29c4dbad.png">
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/160220100-83391805-29ee-45d2-8076-f743c3ba6070.png">
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/160220106-0341a2f4-b4df-4d2b-9ec1-b0f10affd22d.png">
<br/>

## 知识图谱建设方法论
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/218489554-8c49659f-1c52-4b67-9d8d-671d85191d66.png">
<br/>
## 知识图谱（Knowledge Graph）

## 知识图谱建设方法论:
### 一, 知识图谱技术架构: 确定知识的表示方式和知识的存储方式,

### 二, 知识图谱建设方法论: 知识图谱建设可以分为知识建模, 知识抽取, 知识验证这样几个阶段, 形成一个知识图谱
>
>  从知识抽取的内容上, 又可以分为实体抽取, 属性抽取, 关系抽取, 事件抽取:
>
> 实体抽取指从数据源中检测到可命名的实体, 并将它们分类到已建模的类型中, 例如人, 组织, 地点, 时间等等,
>
> 属性抽取是识别出命名实体的具体属性,
>
> 关系抽取是识别出实体与实体之间的关系, 例如从句子“著名歌手周杰伦的妻子昆凌”中识别出“周杰伦”与“昆凌”之间的夫妻关系,
>
> 事件抽取是识别出命名实体相关的事件信息, 例如“周杰伦”与“昆凌”结婚就是一个事件
>
> 可以看出实体抽取, 属性抽取, 关系抽取是抽取我们在知识建模中定义的拓扑结构部分数据
>
> 事件抽取是事件建模相关数据的抽取, 所以在领域知识图谱建设中, 也需要包括数据准备域的抽取方式, 处置域的数据抽取方式
>
> 知 识 验 证
>
> 从各种不同数据源抽取的知识, 并不一定是有效的知识, 必须进行知识的验证, 将有效的, 正确的知识进入知识库造成知识不准确的原因
>
> 通常是原始数据存在错误, 术语存在二义性, 知识冲突等等, 例如前面提到的"1#"压水堆, "1号"压水堆, “一号”压水堆这三个词对应一个实体
>
> 如果在抽取中没有合理定义规则, 这就需要在知识验证阶段得到处理, 以便形成闭环

### 三, 基于知识图谱建设应用: 每一类应用的侧重点不同, 使用技术和达到的效果也不同, 我们总结为知识推理类, 知识呈现类, 知识问答类, 知识共享类

> 1, 知识图谱建设
>
> 1.1 人工数据标注工具: https://github.com/doccano/doccano
>
> 1.2 自动标注+知识抽取: https://github.com/zjunlp/DeepKE
>
> 2, 知识存储: https://github.com/alibaba/GraphScope
>
> 3, 知识图谱应用: https://github.com/lemonhu/stock-knowledge-graph


## 从0到1建设大数据解决方案

> 从0到1建设大数据解决方案是一个相对比较宏观的过程, 需要考虑从业务需求分析, 数据采集, 数据处理, 数据存储, 数据查询分析到数据可视化展示等多个环节,
>
> 以下是一个简单的大数据解决方案建设方法论:
>
> 需求分析: 首先需要明确业务需求, 包括数据源, 数据量, 数据类型, 数据质量等等, 可以与业务人员进行沟通, 制定出明确的需求和目标, 确定解决方案的规模和数据的范围
>
> 数据采集: 根据需求分析结果, 确定数据来源和采集方式, 可以使用采集工具或者开发自定义采集程序, 采集的数据需要进行清洗和过滤, 确保数据的准确性和完整性
>
> 数据处理: 数据采集后需要进行清洗, 整合, 加工等处理, 以便后续的存储和分析, 数据处理可以使用数据流处理或者批处理等方式
>
> 数据存储: 对于大数据解决方案, 数据存储是一个非常重要的环节, 需要选择合适的存储方案, 包括分布式存储, 列式存储, 内存数据库等, 可以根据数据量和查询分析方式等要素进行选择
>
> 数据查询分析: 建立数据查询和分析体系, 需要考虑数据查询和分析的灵活性和效率, 可以使用数据查询引擎和分析工具, 如Hadoop, Spark, Hive, Presto, Superset等
>
> 数据可视化展示: 通过数据可视化展示方式, 使数据分析结果直观, 易于理解, 可以使用开源的可视化工具, 如Tableau, Power BI, Echarts等
>
> 安全与隐私: 对于大数据解决方案, 安全和隐私是非常重要的, 需要采取一系列的安全措施, 包括数据加密, 访问控制, 数据备份等, 以保障数据的安全性和隐私性
>
> 评估和优化: 在建设过程中需要不断评估和优化解决方案, 调整方案架构和技术选型, 以提高解决方案的性能和效率, 满足业务需求和用户期望
>
> 以上是一个简单的从0到1建设大数据解决方案的方法论, 需要根据实际情况进行具体的调整和优化

## 数字化转型
>
> 数字化转型是指将传统企业在信息化, 网络化, 智能化, 数据化等技术的支撑下, 对业务, 组织, 文化, 价值创造, 利益分配等方面进行全面的革新和升级,
>
> 以适应市场, 技术, 用户等环境的变化数字化转型的目标是实现企业从传统生产经营方式向数字化经营模式的转变, 提高企业的效率, 创新能力, 市场竞争力和盈利能力
>
> 数字化转型方法论可以概括为以下几个方面:
>
> 确定数字化转型的战略目标和方向, 明确数字化转型的意义和价值, 为数字化转型的实施提供方向和支撑
>
> 分析业务过程, 识别业务痛点和机会, 确定数字化转型的重点领域和项目, 以提高效率, 创新能力和用户体验为导向
>
> 优化组织结构和流程, 建立数字化组织架构和工作流程, 激发组织创新和员工动力, 提高业务效率和创新能力
>
> 采用先进的信息技术和数据技术, 例如云计算, 大数据, 人工智能, 物联网等, 为数字化转型提供技术支持
>
> 建立数字化文化, 通过数字化营销, 数字化服务, 数字化协同等方式, 提升品牌价值, 用户满意度和市场影响力
>
> 实施数字化监管, 建立数字化安全, 合规和风险控制体系, 确保数字化转型的合法性, 合规性和可持续性
>
> 数字化转型是一个复杂的过程, 需要综合运用战略, 组织, 技术, 文化, 监管等多方面的手段和方法, 才能取得成功



## Community

> 联系作者: https://docs.qq.com/doc/DVFZ1RFhGYkJRSGxN
